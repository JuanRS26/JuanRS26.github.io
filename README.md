# ğŸš€ Data Engineering Pipeline â€“ Sales Analytics

## ğŸ“Œ DescripciÃ³n del Proyecto

Este proyecto implementa un **pipeline de datos end-to-end** simulando un entorno real de Data Engineering.

El pipeline:

- Extrae datos crudos desde archivos CSV
- Limpia y valida los datos
- Aplica transformaciones de negocio
- Carga los datos en un Data Warehouse
- Genera datasets listos para anÃ¡lisis y dashboards

Este proyecto estÃ¡ diseÃ±ado como **proyecto de portafolio profesional** para demostrar buenas prÃ¡cticas de ingenierÃ­a de datos.

---

## ğŸ—ï¸ Arquitectura del Pipeline

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>
â”‚ Fuente CSV  â”‚<br>
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜<br>
       â†“<br>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>
â”‚   Ingesta   â”‚<br>
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜<br>
       â†“<br>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>
â”‚  Limpieza   â”‚<br>
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜<br> 
       â†“<br>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>
â”‚TransformaciÃ³nâ”‚<br>
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜<br>
       â†“<br>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>
â”‚ DataWarehouseâ”‚<br>
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜<br>
       â†“<br>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>
â”‚  Analytics  â”‚<br>
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>

---

## ğŸ”„ Flujo del Pipeline

### 1ï¸âƒ£ Ingesta de Datos

- Fuente: `data/raw/sales.csv`
- Se cargan los datos crudos exactamente como vienen
- No se aplica ninguna transformaciÃ³n

Salida:
data/raw/

---

### 2ï¸âƒ£ Limpieza de Datos

En este paso:

- Se eliminan duplicados
- Se eliminan valores nulos
- Se corrigen tipos de datos
- Se validan reglas bÃ¡sicas de calidad

Salida:
data/clean/

---

### 3ï¸âƒ£ TransformaciÃ³n de Datos

En este paso:

- Se crean mÃ©tricas nuevas:
  - `revenue = price * quantity`
- Se normalizan fechas
- Se generan tablas analÃ­ticas
- Se preparan dimensiones y tabla de hechos

Salida:
data/processed/

---

### 4ï¸âƒ£ Carga al Data Warehouse

- Los datos procesados se cargan en:
  - DuckDB / PostgreSQL / SQLite
- Se crean:
  - Tabla de hechos: `fact_sales`
  - Dimensiones: `dim_products`, `dim_dates`, `dim_customers`

---

### 5ï¸âƒ£ Capa de AnalÃ­tica

Los datos quedan listos para:

- Dashboards
- Consultas SQL
- AnÃ¡lisis exploratorio
- Machine Learning

---

## ğŸ“ Estructura del Proyecto

data-engineering-pipeline/<br>
â”‚<br>
â”œâ”€â”€ data/<br>
â”‚ â”œâ”€â”€ raw/<br>
â”‚ â”œâ”€â”€ clean/<br>
â”‚ â””â”€â”€ processed/<br>
â”‚<br>
â”œâ”€â”€ src/<br>
â”‚ â”œâ”€â”€ ingest.py<br>
â”‚ â”œâ”€â”€ clean.py<br>
â”‚ â”œâ”€â”€ transform.py<br>
â”‚ â””â”€â”€ load.py<br>
â”‚<br>
â”œâ”€â”€ pipelines/<br>
â”‚ â””â”€â”€ main.py<br>
â”‚<br>
â”œâ”€â”€ tests/<br>
â”‚<br>
â”œâ”€â”€ README.md<br>
â””â”€â”€ requirements.txt<br>

---

## â–¶ï¸ CÃ³mo Ejecutar el Pipeline

Ejecutar todo el pipeline:

python pipelines/main.py
O ejecutar por pasos:

bash<br>
Copiar cÃ³digo<br>
python src/ingest.py<br>
python src/clean.py<br>
python src/transform.py<br>
python src/load.py<br>

ğŸ› ï¸ TecnologÃ­as Utilizadas<br>
Python
Pandas o Polars
SQL
DuckDB / PostgreSQL / SQLite
(Opcional en el futuro)
Airflow o Prefect
Docker

ğŸ“Š Ejemplo del Dataset Final
Tabla: fact_sales

date	product	country	quantity	revenue<br>
2024-01-01	Laptop	MX	2	2400<br>
2024-01-02	Mouse	CO	5	250<br>

ğŸ§  Principios de DiseÃ±o<br>

Este pipeline fue diseÃ±ado para ser:<br>

âœ… Modular<br>
âœ… Reproducible<br>
âœ… Escalable<br>
âœ… FÃ¡cil de mantener<br>
âœ… FÃ¡cil de orquestar en producciÃ³n<br>

Cada etapa:<br>

Tiene inputs y outputs bien definidos<br>
Puede ejecutarse de forma independiente<br>
Puede integrarse con Airflow, Prefect o Dagster<br>

ğŸ† Objetivo del Proyecto<br>

Este proyecto demuestra:<br>
Buenas prÃ¡cticas de Data Engineering<br>
DiseÃ±o de pipelines reales<br>
OrganizaciÃ³n de proyectos de datos<br>
SeparaciÃ³n por capas (raw / clean / processed)<br>
Carga en Data Warehouse<br>
PreparaciÃ³n de datos analÃ­ticos<br>

ğŸš€ Roadmap<br>

 Agregar orquestaciÃ³n con Airflow o Prefect<br>
 Agregar validaciones con Great Expectations<br>
 Agregar tests automÃ¡ticos<br>
 Dockerizar el proyecto<br>
 Agregar CI/CD<br>

ğŸ‘¤ Autor<br>
Tu Nombre AquÃ­<br>
Data Engineer / Analytics Engineer<br>

â­ Si te gustÃ³ este proyecto
No olvides darle una estrella â­ al repo ğŸ˜„
